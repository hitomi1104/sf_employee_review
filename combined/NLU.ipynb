{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bd1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import warnings\n",
    "import streamlit as st\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "\n",
    "# this line tells jupyter notebook to put the plots in the notebook rather than saving them to file\n",
    "%matplotlib inline\n",
    "\n",
    "# this line makes plots prettier on mac retina screens - if you don't have one it shouldn't do anything\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# Style\n",
    "sns.set_style(style = 'darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec40ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/nlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0d640d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns = ['rating', 'Unnamed: 0', 'rating.1', 'pros_nlp',\n",
    "       'cons_nlp'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b26b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import re,string,unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d623e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',str(text))\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['pros']=df['pros'].apply(remove_special_characters)\n",
    "df['cons']=df['cons'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f613ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "\n",
    "#Apply function on review column\n",
    "df['pros']=df['pros'].apply(lemmatize_text)\n",
    "df['cons']=df['cons'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e634c6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "df['pros']=df['pros'].apply(remove_stopwords)\n",
    "df['cons']=df['cons'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61eeb7a",
   "metadata": {},
   "source": [
    "### Pros\n",
    "#### Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joy = df[df['emotion+'] == 'joy' ]\n",
    "surprise = df[df['emotion+'] == 'surprise' ]\n",
    "sadness = df[df['emotion+'] == 'sadness' ]\n",
    "fear = df[df['emotion+'] == 'fear' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(joy['pros'])\n",
    "\n",
    "pro_tf = tf.transform(joy['pros'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(surprise['pros'])\n",
    "\n",
    "pro_tf = tf.transform(surprise['pros'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0addba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(sadness['pros'])\n",
    "\n",
    "pro_tf = tf.transform(sadness['pros'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d360272",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(fear['pros'])\n",
    "\n",
    "pro_tf = tf.transform(fear['pros'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff000d",
   "metadata": {},
   "source": [
    "### Cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "joy = df[df['emotion-'] == 'joy' ]\n",
    "surprise = df[df['emotion-'] == 'surprise' ]\n",
    "sadness = df[df['emotion-'] == 'sadness' ]\n",
    "fear = df[df['emotion-'] == 'fear' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f493595",
   "metadata": {},
   "source": [
    "#### Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d953114",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(joy['cons'])\n",
    "\n",
    "pro_tf = tf.transform(joy['cons'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ce5d1b",
   "metadata": {},
   "source": [
    "#### Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99adc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(surprise['cons'])\n",
    "\n",
    "pro_tf = tf.transform(surprise['cons'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586d7c3",
   "metadata": {},
   "source": [
    "#### Sadness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(sadness['cons'])\n",
    "\n",
    "pro_tf = tf.transform(sadness['cons'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602cb7f",
   "metadata": {},
   "source": [
    "#### Fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b9f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', min_df=3, ngram_range = (2,3))\n",
    "tf.fit(fear['cons'])\n",
    "\n",
    "pro_tf = tf.transform(fear['cons'])\n",
    "pro_df = pd.DataFrame(pro_tf.todense(), columns=tf.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "top_texts = pro_df.sum().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "Cloud = WordCloud(width=1000, height=700,\n",
    "                  background_color='white',\n",
    "                  stopwords=stopwords,\n",
    "                  min_font_size=3,\n",
    "                  min_word_length=0).generate_from_frequencies(top_texts.head(100))\n",
    "\n",
    "# background_color=\"white\", max_words=50).generate_from_frequencies(top_texts)\n",
    "\n",
    "# Display the generated image:\n",
    "# plt.figure(figsize=(15,10))\n",
    "plt.imshow(Cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "print(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6964fffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2504b303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f601525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

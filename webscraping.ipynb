{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e705e55",
   "metadata": {},
   "source": [
    "https://bulletbyte.weebly.com/tech/how-to-scrape-a-companys-glassdoor-reviews-using-python\n",
    "\n",
    "https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ded2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4923e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to scrape any Glassdoor company review page\n",
    "#the code still works when I run it on 7 Sep, 2021, but the html content of Glassdoor webpages changes all the time\n",
    "#please inspect the webpage and make the necessary changes to the html tags if any of the list returns empty\n",
    "\n",
    "def review_scraper(url):\n",
    "  #scraping the web page content\n",
    "  hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "  req = Request(url,headers=hdr)\n",
    "  page = urlopen(req)\n",
    "  soup = BeautifulSoup(page, \"html.parser\") \n",
    "\n",
    "  #define some lists\n",
    "  Summary=[]\n",
    "  Date_n_JobTitle=[]\n",
    "  Date=[]\n",
    "  JobTitle=[]\n",
    "  AuthorLocation=[]\n",
    "  OverallRating=[]\n",
    "  Pros=[]\n",
    "  Cons=[]  \n",
    "\n",
    "  #get the Summary\n",
    "  for x in soup.find_all('h2', {'class':'mb-xxsm mt-0 css-93svrw el6ke055'}):\n",
    "    Summary.append(x.text)\n",
    "\n",
    "  #get the Posted Date and Job Title\n",
    "  for x in soup.find_all('span', {'class':'middle common__EiReviewDetailsStyle__newGrey'}):\n",
    "    Date_n_JobTitle.append(x.text)\n",
    "\n",
    "  #get the Posted Date\n",
    "  for x in Date_n_JobTitle:\n",
    "    Date.append(x.split(' -')[0])\n",
    "\n",
    "  #get Job Title\n",
    "  for x in Date_n_JobTitle:\n",
    "    JobTitle.append(x.split(' -')[1])\n",
    "\n",
    "  #get Author Location\n",
    "  for x in soup.find_all('span', {'class':'middle'}):\n",
    "    AuthorLocation.append(x.text)\n",
    "\n",
    "  #get Overall Rating\n",
    "  for x in soup.find_all('span', {'class':'ratingNumber mr-xsm'}):\n",
    "    OverallRating.append(float(x.text))\n",
    "\n",
    "  #get Pros\n",
    "  for x in soup.find_all('span', {'data-test':'pros'}):\n",
    "    Pros.append(x.text)\n",
    "\n",
    "  #get Cons\n",
    "  for x in soup.find_all('span', {'data-test':'cons'}):\n",
    "    Cons.append(x.text)\n",
    "\n",
    "  #putting everything together\n",
    "  Reviews = pd.DataFrame(list(zip(Summary, Date, JobTitle, AuthorLocation, OverallRating, Pros, Cons)), \n",
    "                    columns = ['Summary', 'Date', 'JobTitle', 'AuthorLocation', 'OverallRating', 'Pros', 'Cons'])\n",
    "  \n",
    "  return Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84b814",
   "metadata": {},
   "source": [
    "<h2 class=\"mb-xxsm mt-0 css-93svrw el6ke055\"><a href=\"/Reviews/Employee-Review-Salesforce-RVW51057878.htm\" class=\"reviewLink\">Amazing!</a></h2>\n",
    "<span class=\"middle common__EiReviewDetailsStyle__newGrey\">Nov 30, 2020 - Account Executive- Core Team</span>\n",
    "<span class=\"middle\">in <span>San Francisco, CA</span></span>\n",
    "<span class=\"ratingNumber mr-xsm\">3.0</span>\n",
    "<span data-test=\"pros\">- Benefits are top notch\n",
    "- Perks in the tower and holiday party are impressive\n",
    "- Sales tactics and strategies are great for growth even as an experienced rep\n",
    "- Youâ€™ll meet very talented sales rep with a wide variance of styles\n",
    "-ESPP\n",
    "- generous maternity/paternity leave. Although this will affect your likelihood of be promoted</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159   .htm\n",
    "# https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159   _P2.htm?filter.iso3Language=eng\n",
    "# https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159   _P2.htm?filter.iso3Language=eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7cb70e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m maxPage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# maxPage = 1770 + 1\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#uncomment the line below to set the max page to scrape (based on total number of reviews)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#maxPage = countPages + 1\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#scraping multiple pages of company glassdoor review\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# output = review_scraper(input_url+str(1)+\".htm?sort.sortType=RD&sort.ascending=false\")\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mreview_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_url\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_P\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtm?filter.iso3Language=eng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,maxPage):\n\u001b[1;32m     32\u001b[0m     url \u001b[38;5;241m=\u001b[39m input_url\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_P\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtm?filter.iso3Language=eng\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mreview_scraper\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      7\u001b[0m hdr \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      8\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url,headers\u001b[38;5;241m=\u001b[39mhdr)\n\u001b[0;32m----> 9\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#define some lists\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    530\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    568\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#paste/replace the url to the first page of the company's Glassdoor review in between the \"\"\n",
    "\n",
    "\n",
    "input_url=\"https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159\"\n",
    "# https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159_P3.htm?filter.iso3Language=eng\n",
    "\n",
    "#scraping the first page content\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = Request(input_url+\".htm\",headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, \"html.parser\") \n",
    "\n",
    "#check the total number of reviews\n",
    "countReviews = soup.find('div', {'data-test':'pagination-footer-text'}).text\n",
    "countReviews = float(countReviews.split(' Reviews')[0].split('of ')[1].replace(',',''))\n",
    "\n",
    "#calculate the max number of pages (assuming 10 reviews a page)\n",
    "countPages = math.ceil(countReviews/10)\n",
    "countPages\n",
    "\n",
    "#I'm setting the max pages to scrape to 3 here to save time\n",
    "maxPage = 3 + 1\n",
    "# maxPage = 1770 + 1\n",
    "#uncomment the line below to set the max page to scrape (based on total number of reviews)\n",
    "#maxPage = countPages + 1\n",
    "\n",
    "#scraping multiple pages of company glassdoor review\n",
    "# output = review_scraper(input_url+str(1)+\".htm?sort.sortType=RD&sort.ascending=false\")\n",
    "output = review_scraper(input_url+\"_P\"+str(x)+\"htm?filter.iso3Language=eng\")\n",
    "\n",
    "for x in range(2,maxPage):\n",
    "    url = input_url+\"_P\"+str(x)+\"htm?filter.iso3Language=eng\"\n",
    "    output = output.append(review_scraper(url), ignore_index=True)\n",
    "    print(x)\n",
    "#     time.sleep(1)\n",
    "\n",
    "#display the output\n",
    "display(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9835d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b88284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e705e55",
   "metadata": {},
   "source": [
    "https://bulletbyte.weebly.com/tech/how-to-scrape-a-companys-glassdoor-reviews-using-python\n",
    "\n",
    "https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ded2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4923e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to scrape any Glassdoor company review page\n",
    "#the code still works when I run it on 7 Sep, 2021, but the html content of Glassdoor webpages changes all the time\n",
    "#please inspect the webpage and make the necessary changes to the html tags if any of the list returns empty\n",
    "\n",
    "def review_scraper(url):\n",
    "  #scraping the web page content\n",
    "  hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "  req = Request(url,headers=hdr)\n",
    "  page = urlopen(req)\n",
    "  soup = BeautifulSoup(page, \"html.parser\") \n",
    "\n",
    "  #define some lists\n",
    "  Summary=[]\n",
    "  Date_n_JobTitle=[]\n",
    "  Date=[]\n",
    "  JobTitle=[]\n",
    "  AuthorLocation=[]\n",
    "  OverallRating=[]\n",
    "  Pros=[]\n",
    "  Cons=[]  \n",
    "\n",
    "  #get the Summary\n",
    "  for x in soup.find_all('h2', {'class':'mb-xxsm mt-0 css-93svrw el6ke055'}):\n",
    "    Summary.append(x.text)\n",
    "\n",
    "  #get the Posted Date and Job Title\n",
    "  for x in soup.find_all('span', {'class':'middle common__EiReviewDetailsStyle__newGrey'}):\n",
    "    Date_n_JobTitle.append(x.text)\n",
    "\n",
    "  #get the Posted Date\n",
    "  for x in Date_n_JobTitle:\n",
    "    Date.append(x.split(' -')[0])\n",
    "\n",
    "  #get Job Title\n",
    "  for x in Date_n_JobTitle:\n",
    "    JobTitle.append(x.split(' -')[1])\n",
    "\n",
    "  #get Author Location\n",
    "  for x in soup.find_all('span', {'class':'middle'}):\n",
    "    AuthorLocation.append(x.text)\n",
    "\n",
    "  #get Overall Rating\n",
    "  for x in soup.find_all('span', {'class':'ratingNumber mr-xsm'}):\n",
    "    OverallRating.append(float(x.text))\n",
    "\n",
    "  #get Pros\n",
    "  for x in soup.find_all('span', {'data-test':'pros'}):\n",
    "    Pros.append(x.text)\n",
    "\n",
    "  #get Cons\n",
    "  for x in soup.find_all('span', {'data-test':'cons'}):\n",
    "    Cons.append(x.text)\n",
    "\n",
    "  #putting everything together\n",
    "  Reviews = pd.DataFrame(list(zip(Summary, Date, JobTitle, AuthorLocation, OverallRating, Pros, Cons)), \n",
    "                    columns = ['Summary', 'Date', 'JobTitle', 'AuthorLocation', 'OverallRating', 'Pros', 'Cons'])\n",
    "  \n",
    "  return Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84b814",
   "metadata": {},
   "source": [
    "<h2 class=\"mb-xxsm mt-0 css-93svrw el6ke055\"><a href=\"/Reviews/Employee-Review-Salesforce-RVW51057878.htm\" class=\"reviewLink\">Amazing!</a></h2>\n",
    "<span class=\"middle common__EiReviewDetailsStyle__newGrey\">Nov 30, 2020 - Account Executive- Core Team</span>\n",
    "<span class=\"middle\">in <span>San Francisco, CA</span></span>\n",
    "<span class=\"ratingNumber mr-xsm\">3.0</span>\n",
    "<span data-test=\"pros\">- Benefits are top notch\n",
    "- Perks in the tower and holiday party are impressive\n",
    "- Sales tactics and strategies are great for growth even as an experienced rep\n",
    "- Youâ€™ll meet very talented sales rep with a wide variance of styles\n",
    "-ESPP\n",
    "- generous maternity/paternity leave. Although this will affect your likelihood of be promoted</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cb70e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(0 bytes read)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:555\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:522\u001b[0m, in \u001b[0;36mHTTPResponse._read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;66;03m# close the connection as protocol synchronisation is\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# probably lost\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 16: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:572\u001b[0m, in \u001b[0;36mHTTPResponse._readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m     chunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:557\u001b[0m, in \u001b[0;36mHTTPResponse._get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;66;03m# last chunk: 1*(\"0\") [ chunk-extension ] CRLF\u001b[39;00m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(0 bytes read)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,maxPage):\n\u001b[1;32m     27\u001b[0m     url \u001b[38;5;241m=\u001b[39m input_url\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_P\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(x)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.htm?sort.sortType=RD&sort.ascending=false\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mappend(\u001b[43mreview_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#display the output\u001b[39;00m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mreview_scraper\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      8\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url,headers\u001b[38;5;241m=\u001b[39mhdr)\n\u001b[1;32m      9\u001b[0m page \u001b[38;5;241m=\u001b[39m urlopen(req)\n\u001b[0;32m---> 10\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#define some lists\u001b[39;00m\n\u001b[1;32m     13\u001b[0m Summary\u001b[38;5;241m=\u001b[39m[]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/bs4/__init__.py:309\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     markup \u001b[38;5;241m=\u001b[39m \u001b[43mmarkup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(markup) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    311\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# just in case that's what the user really wants.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msupports_unicode_filenames):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked:\n\u001b[0;32m--> 465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readall_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py:579\u001b[0m, in \u001b[0;36mHTTPResponse._readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(value)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(value))\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(0 bytes read)"
     ]
    }
   ],
   "source": [
    "#paste/replace the url to the first page of the company's Glassdoor review in between the \"\"\n",
    "input_url=\"https://www.glassdoor.com/Reviews/Salesforce-Reviews-E11159.htm?filter.iso3Language=eng\"\n",
    "\n",
    "#scraping the first page content\n",
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = Request(input_url+str(1)+\".htm?sort.sortType=RD&sort.ascending=false\",headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, \"html.parser\") \n",
    "\n",
    "#check the total number of reviews\n",
    "countReviews = soup.find('div', {'data-test':'pagination-footer-text'}).text\n",
    "countReviews = float(countReviews.split(' Reviews')[0].split('of ')[1].replace(',',''))\n",
    "\n",
    "#calculate the max number of pages (assuming 10 reviews a page)\n",
    "countPages = math.ceil(countReviews/10)\n",
    "countPages\n",
    "\n",
    "#I'm setting the max pages to scrape to 3 here to save time\n",
    "# maxPage = 3 + 1\n",
    "maxPage = 1770 + 1\n",
    "#uncomment the line below to set the max page to scrape (based on total number of reviews)\n",
    "#maxPage = countPages + 1\n",
    "\n",
    "#scraping multiple pages of company glassdoor review\n",
    "output = review_scraper(input_url+str(1)+\".htm?sort.sortType=RD&sort.ascending=false\")\n",
    "for x in range(2,maxPage):\n",
    "    url = input_url+\"_P\"+str(x)+\".htm?sort.sortType=RD&sort.ascending=false\"\n",
    "    output = output.append(review_scraper(url), ignore_index=True)\n",
    "    time.sleep(3)\n",
    "\n",
    "#display the output\n",
    "display(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b88284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

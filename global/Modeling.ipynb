{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2204e136",
   "metadata": {},
   "source": [
    "reference : https://www.kaggle.com/code/heeraldedhia/text-classification-nlp/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c73f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hitomihoshino/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hitomihoshino/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hitomihoshino/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/hitomihoshino/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import re,string,unicodedata\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Importing the basic librarires for building model - classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import  MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc87e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/glassdoor_webscraped.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb6c648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df = df.loc[:, \"rating\":\"cons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc5c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9888 entries, 0 to 9887\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   rating  9887 non-null   float64\n",
      " 1   pros    9887 non-null   object \n",
      " 2   cons    9887 non-null   object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 231.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a636f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af840d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pros'] = df['pros'].str.lower()\n",
    "df['cons'] = df['cons'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc93e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd5dd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myself', 'where', 'ma', 'other', 'we', 'an', 'me', 'doing', 'or', 'up', 's', 'didn', 'are', 'their', \"you'll\", 'no', 'same', 'about', \"don't\", \"it's\", 'yourself', 'don', 'below', 'because', 'so', 'mustn', 'yourselves', 'only', 'isn', 'have', 'hasn', \"you'd\", 'down', 'own', 'aren', 'why', 'having', 'my', 're', 'yours', 'here', 'from', 'few', \"wasn't\", 'theirs', 'into', 'further', 'in', 'wouldn', 'if', 'but', 'can', 'once', 'whom', 'doesn', 'until', 'when', 'than', 'during', \"won't\", 'at', 'you', 'they', 'out', 'while', 'just', \"shouldn't\", 'the', \"you've\", 'ours', 'these', 'o', 'on', \"wouldn't\", 'herself', 't', \"shan't\", 'what', 'after', 'under', 'he', 'such', \"couldn't\", 'as', 'for', 'haven', \"haven't\", 'both', 'weren', 'which', 'how', 'will', 'his', 'do', 'that', 'any', 'them', \"didn't\", 'of', 'to', \"should've\", 'll', 'this', 'she', 'has', 'd', \"she's\", 'more', \"hasn't\", 'ain', 'm', 'couldn', 'it', \"that'll\", 'above', 'wasn', \"weren't\", 'themselves', 'ourselves', 'off', 'your', 'y', \"needn't\", 'nor', 'am', 'hadn', 'i', 'did', 'won', 'had', 'her', 'against', \"isn't\", 'who', \"hadn't\", 'by', 'before', 'with', 'shouldn', \"mustn't\", 'now', 'between', 'were', 'hers', 'should', 'its', 'been', 've', 'there', \"mightn't\", 'then', 'was', \"you're\", 'himself', 'itself', 'a', 'be', 'each', 'does', 'most', 'some', 'too', 'our', 'him', \"aren't\", 'all', 'mightn', 'through', 'and', 'being', 'again', 'over', 'is', \"doesn't\", 'very', 'shan', 'those', 'needn', 'not'}\n"
     ]
    }
   ],
   "source": [
    "#set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#Apply function on review column\n",
    "df['pros']=df['pros'].apply(remove_stopwords)\n",
    "df['cons']=df['cons'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a98b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',str(text))\n",
    "    return text\n",
    "#Apply function on review column\n",
    "df['pros']=df['pros'].apply(remove_special_characters)\n",
    "df['cons']=df['cons'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf142c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "\n",
    "#Apply function on review column\n",
    "df['pros']=df['pros'].apply(lemmatize_text)\n",
    "df['cons']=df['cons'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f574b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pros'] = [','.join(map(str, l)) for l in df['pros']]\n",
    "df['cons'] = [','.join(map(str, l)) for l in df['cons']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cea2589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>pros</th>\n",
       "      <th>cons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>leader,support,transparency,benefit,worklife,b...</td>\n",
       "      <td>found,con,yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>quickly,move,access,networking,people,position...</td>\n",
       "      <td>large,company,decent,amount,red,tape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>high,income,potential,upward,mobility</td>\n",
       "      <td>pressure,cooker,high,expectation,stress,expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>excellent,growth,networking,opportunity</td>\n",
       "      <td>many,con,think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>given,freedom,flexibility,explore,multiple,rol...</td>\n",
       "      <td>still,startup,mentality,keep,mind,entail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                               pros  \\\n",
       "0     5.0  leader,support,transparency,benefit,worklife,b...   \n",
       "1     5.0  quickly,move,access,networking,people,position...   \n",
       "2     5.0              high,income,potential,upward,mobility   \n",
       "3     5.0            excellent,growth,networking,opportunity   \n",
       "4     5.0  given,freedom,flexibility,explore,multiple,rol...   \n",
       "\n",
       "                                               cons  \n",
       "0                                     found,con,yet  \n",
       "1              large,company,decent,amount,red,tape  \n",
       "2  pressure,cooker,high,expectation,stress,expected  \n",
       "3                                    many,con,think  \n",
       "4          still,startup,mentality,keep,mind,entail  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445af28b",
   "metadata": {},
   "source": [
    "### Pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ffd7575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = list(df['pros'])\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features = 20000) \n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "docs = tfidf_vectorizer_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67dde98d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sinking', 'siprit', 'sit', 'site', 'sitstand', 'sitting', 'situated', 'situation', 'size', 'sizeable', 'sized', 'skeptic', 'skeptical', 'ski', 'skill', 'skillbuilding', 'skilled', 'skillset', 'skillsets', 'skimp']\n",
      "\n",
      "6092\n"
     ]
    }
   ],
   "source": [
    "# Lets use the stop_words argument to remove words like \"and, the, a\"\n",
    "tfvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit our vectorizer using our train data\n",
    "tfvec.fit(df['pros'])\n",
    "\n",
    "# Transform training data\n",
    "tfvec_mat = tfvec.transform(df['pros'])\n",
    "\n",
    "# words occuring\n",
    "words = tfvec.get_feature_names()\n",
    "print(words[5000:5020])\n",
    "print()\n",
    "# number of different words\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1040ad9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100k',\n",
       " '100mo',\n",
       " '100month',\n",
       " '100mth',\n",
       " '1010',\n",
       " '1012',\n",
       " '1013',\n",
       " '105',\n",
       " '10b',\n",
       " '10k',\n",
       " '11',\n",
       " '110',\n",
       " '111',\n",
       " '11th',\n",
       " '12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfvec.get_feature_names()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "934ccde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>836.290865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>618.496469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>604.409813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>532.648889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benefit</th>\n",
       "      <td>514.056560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>492.626671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>404.982177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>277.395927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opportunity</th>\n",
       "      <td>253.003399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employee</th>\n",
       "      <td>240.875073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>229.523933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>228.153288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>225.970377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>225.612977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay</th>\n",
       "      <td>219.931610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot</th>\n",
       "      <td>219.116851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>team</th>\n",
       "      <td>210.663577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environment</th>\n",
       "      <td>204.297515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>191.530957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salesforce</th>\n",
       "      <td>185.991969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "great        836.290865\n",
       "good         618.496469\n",
       "work         604.409813\n",
       "culture      532.648889\n",
       "benefit      514.056560\n",
       "company      492.626671\n",
       "people       404.982177\n",
       "balance      277.395927\n",
       "opportunity  253.003399\n",
       "employee     240.875073\n",
       "product      229.523933\n",
       "amazing      228.153288\n",
       "place        225.970377\n",
       "life         225.612977\n",
       "pay          219.931610\n",
       "lot          219.116851\n",
       "team         210.663577\n",
       "environment  204.297515\n",
       "best         191.530957\n",
       "salesforce   185.991969"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tfvec_mat.sum(axis=0)\n",
    "pd.DataFrame(a, columns=words).transpose().sort_values(by=0, ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d52cb9",
   "metadata": {},
   "source": [
    "## Predicting Using Pro column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0c6d07",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8a30f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.value_counts(df['rating']).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca8577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9887, 6092) (9887,)\n"
     ]
    }
   ],
   "source": [
    "X = tfvec_mat \n",
    "y = df['rating']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b10a4718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7909, 6092) (7909,)\n",
      "(1978, 6092) (1978,)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "SEED=123\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd8a21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2974eb74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.6471108863320268\n",
      "Testing score:  0.5819009100101112\n",
      "\n",
      "Cross-validated scores: [0.58695652 0.58796764 0.58573596 0.60748609 0.57561963]\n",
      "Average cv score: 0.5887531690272307\n",
      "Standard deviation of cv score: 0.010357227520543712\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "lr.fit(X_train, y_train)\n",
    "cv = cross_val_score(lr, X, y, cv=5)\n",
    "\n",
    "print('Training score: ', lr.score(X_train, y_train))\n",
    "print('Testing score: ', lr.score(X_test, y_test))\n",
    "\n",
    "print()\n",
    "print('Cross-validated scores:', cv)\n",
    "print('Average cv score:', cv.mean())\n",
    "print('Standard deviation of cv score:', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4ea483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.9911493235554432\n",
      "Testing score:  0.6678463094034378\n",
      "\n",
      "Cross-validated scores: [0.66182048 0.65992415 0.66498104 0.65613148 0.6514864 ]\n",
      "Average cv score: 0.6588687087738321\n",
      "Standard deviation of cv score: 0.004672865310377553\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "rfc = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "rfc.fit(X_train, y_train)\n",
    "cv = cross_val_score(rfc, X_train, y_train, cv=5)\n",
    "\n",
    "print('Training score: ', rfc.score(X_train, y_train))\n",
    "print('Testing score: ', rfc.score(X_test, y_test))\n",
    "\n",
    "print()\n",
    "print('Cross-validated scores:', cv)\n",
    "print('Average cv score:', cv.mean())\n",
    "print('Standard deviation of cv score:', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91b4416f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46112024276141106\n",
      "0.31193124368048536\n",
      "\n",
      "Cross-validated scores: [0.32406471 0.49443883 0.47749115 0.31562974 0.34294385]\n",
      "Average score: 0.3909136566981357\n",
      "Standard deviation of score: 0.07829517752557197\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbors\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "cv = cross_val_score(knn, X, y, cv=5)\n",
    "\n",
    "print(knn.score(X_train, y_train))\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "print()\n",
    "print('Cross-validated scores:', cv)\n",
    "print('Average score:', cv.mean())\n",
    "print('Standard deviation of score:', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d014609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Naive Bayes Classifier\n",
    "# #### Gaussian Naive Bayes\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb = GaussianNB() \n",
    "# gnb.fit(X_train, y_train)\n",
    "# cv = cross_val_score(gnb, X, y, cv=5)\n",
    "\n",
    "# print(gnb.score(X_train, y_train))\n",
    "# print(gnb.score(X_test, y_test))\n",
    "\n",
    "# print()\n",
    "# print('Cross-validated scores:', cv)\n",
    "# print('Average score:', cv.mean())\n",
    "# print('Standard deviation of score:', cv.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed1162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e68ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e29ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f65a0b",
   "metadata": {},
   "source": [
    "reference : https://www.datacamp.com/cheat-sheet/scikit-learn-cheat-sheet-python-machine-learning\n",
    "* https://www.datacamp.com/cheat-sheet/machine-learning-cheat-sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208b6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
